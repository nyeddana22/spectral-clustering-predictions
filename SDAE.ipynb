{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EMYh7L3r1xRI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import pickle\n",
    "import torch.utils.data as data\n",
    "import codecs\n",
    "import urllib\n",
    "class Netflix(data.Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.training_file = \"/Users/niharika/Documents/Niharika/Third Quarter/ECS 271 ML/Assignment2/user_pred_matrix.npy\" # your data for clustering (e.g., user-predictions matrix, user-embedding learned from SVD ...)\n",
    "        self.label_file = \"/Users/niharika/Documents/Niharika/Third Quarter/ECS 271 ML/Assignment2/labels.npy\" # clustering labels (not applicable for our assignment, I used the k-means labels for svd_pu file)\n",
    "        if download:\n",
    "            self.download()\n",
    "        train = np.load(self.training_file)\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        label = np.load(self.label_file)\n",
    "        train = scaler.fit_transform(train)\n",
    "        if self.train:\n",
    "            self.train_data, self.train_labels = torch.tensor(train, dtype=torch.float32), torch.tensor(label, dtype=torch.int)\n",
    "            if self.use_cuda:\n",
    "                self.train_data = self.train_data.cuda()\n",
    "                self.train_labels = self.train_labels.cuda()\n",
    "        else:\n",
    "            self.test_data, self.test_labels = torch.tensor(train, dtype=torch.float32), torch.tensor(label, dtype=torch.int)\n",
    "            if self.use_cuda:\n",
    "                self.test_data = self.test_data.cuda()\n",
    "                self.test_labels = self.test_labels.cuda()\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            img, target = self.test_data[index], self.test_labels[index]\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fQT5YNTI2l5Q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, data, labels, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        if torch.cuda.is_available():\n",
    "            self.data = self.data.cuda()\n",
    "            self.labels = self.labels.cuda()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.labels[index]\n",
    "        # img = Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def masking_noise(data, frac):\n",
    "    \"\"\"\n",
    "    data: Tensor\n",
    "    frac: fraction of unit to be masked out\n",
    "    \"\"\"\n",
    "    data_noise = data.clone()\n",
    "    rand = torch.rand(data.size())\n",
    "    data_noise[rand<frac] = 0\n",
    "    return data_noise\n",
    "\n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return 0.5 * torch.mean((input-target)**2)\n",
    "\n",
    "class BCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return -torch.mean(torch.sum(target*torch.log(torch.clamp(input, min=1e-10))+\n",
    "            (1-target)*torch.log(torch.clamp(1-input, min=1e-10)), 1))\n",
    "        \n",
    "\n",
    "def adjust_learning_rate(init_lr, optimizer, epoch):\n",
    "    lr = init_lr * (0.1 ** (epoch//100))\n",
    "    toprint = True\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group[\"lr\"]!=lr:\n",
    "            param_group[\"lr\"] = lr\n",
    "            if toprint:\n",
    "                print(\"Switching to learning rate %f\" % lr)\n",
    "                toprint = False\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activation=\"relu\", \n",
    "        dropout=0.2, tied=False):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if tied:\n",
    "            self.deweight = self.weight.t()\n",
    "        else:\n",
    "            self.deweight = Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.bias = Parameter(torch.Tensor(out_features))\n",
    "        self.vbias = Parameter(torch.Tensor(in_features))\n",
    "        \n",
    "        if activation==\"relu\":\n",
    "            self.enc_act_func = nn.ReLU()\n",
    "        elif activation==\"sigmoid\":\n",
    "            self.enc_act_func = nn.Sigmoid()\n",
    "        elif activation==\"none\":\n",
    "            self.enc_act_func = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 0.01\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        stdv = 0.01\n",
    "        self.deweight.data.uniform_(-stdv, stdv)\n",
    "        self.vbias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.enc_act_func is not None:\n",
    "            return self.dropout(self.enc_act_func(F.linear(x, self.weight, self.bias)))\n",
    "        else:\n",
    "            return self.dropout(F.linear(x, self.weight, self.bias))\n",
    "\n",
    "    def encode(self, x, train=True):\n",
    "        if train:\n",
    "            self.dropout.train()\n",
    "        else:\n",
    "            self.dropout.eval()\n",
    "        if self.enc_act_func is not None:\n",
    "            return self.dropout(self.enc_act_func(F.linear(x, self.weight, self.bias)))\n",
    "        else:\n",
    "            return self.dropout(F.linear(x, self.weight, self.bias))\n",
    "\n",
    "    def encodeBatch(self, dataloader):\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        encoded = []\n",
    "        for batch_idx, (inputs, _) in enumerate(dataloader):\n",
    "            inputs = inputs.view(inputs.size(0), -1).float()\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            inputs = Variable(inputs)\n",
    "            hidden = self.encode(inputs, train=False)\n",
    "            encoded.append(hidden.data.cpu())\n",
    "\n",
    "        encoded = torch.cat(encoded, dim=0)\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, x, binary=False):\n",
    "        if not binary:\n",
    "            return F.linear(x, self.deweight, self.vbias)\n",
    "        else:\n",
    "            return F.sigmoid(F.linear(x, self.deweight, self.vbias))\n",
    "\n",
    "    def fit(self, trainloader, validloader, lr=0.001, batch_size=128, num_epochs=10, corrupt=0.3,\n",
    "        loss_type=\"mse\"):\n",
    "        \"\"\"\n",
    "        data_x: FloatTensor\n",
    "        valid_x: FloatTensor\n",
    "        \"\"\"\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        print(\"=====Denoising Autoencoding layer=======\")\n",
    "        # optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.parameters()), lr=lr, momentum=0.9)\n",
    "        if loss_type==\"mse\":\n",
    "            criterion = MSELoss()\n",
    "        elif loss_type==\"cross-entropy\":\n",
    "            criterion = BCELoss()\n",
    "\n",
    "        # validate\n",
    "        total_loss = 0.0\n",
    "        total_num = 0\n",
    "        for batch_idx, (inputs, _) in enumerate(validloader):\n",
    "            # inputs = inputs.view(inputs.size(0), -1).float()\n",
    "            # if use_cuda:\n",
    "            #     inputs = inputs.cuda()\n",
    "            inputs = Variable(inputs)\n",
    "            hidden = self.encode(inputs)\n",
    "            if loss_type==\"cross-entropy\":\n",
    "                outputs = self.decode(hidden, binary=True)\n",
    "            else:\n",
    "                outputs = self.decode(hidden)\n",
    "\n",
    "            valid_recon_loss = criterion(outputs, inputs)\n",
    "            total_loss += valid_recon_loss.data * len(inputs)\n",
    "            total_num += inputs.size()[0]\n",
    "\n",
    "        valid_loss = total_loss / total_num\n",
    "        print(\"#Epoch 0: Valid Reconstruct Loss: %.4f\" % (valid_loss))\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            # train 1 epoch\n",
    "            train_loss = 0.0\n",
    "            adjust_learning_rate(lr, optimizer, epoch)\n",
    "            for batch_idx, (inputs, _) in enumerate(trainloader):\n",
    "                # inputs = inputs.view(inputs.size(0), -1).float()\n",
    "                inputs_corr = masking_noise(inputs, corrupt)\n",
    "                # if use_cuda:\n",
    "                #     inputs = inputs.cuda()\n",
    "                #     inputs_corr = inputs_corr.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                inputs = Variable(inputs)\n",
    "                inputs_corr = Variable(inputs_corr)\n",
    "\n",
    "                hidden = self.encode(inputs_corr)\n",
    "                if loss_type==\"cross-entropy\":\n",
    "                    outputs = self.decode(hidden, binary=True)\n",
    "                else:\n",
    "                    outputs = self.decode(hidden)\n",
    "                recon_loss = criterion(outputs, inputs)\n",
    "                train_loss += recon_loss.data*len(inputs)\n",
    "                recon_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # validate\n",
    "            valid_loss = 0.0\n",
    "            for batch_idx, (inputs, _) in enumerate(validloader):\n",
    "                # inputs = inputs.view(inputs.size(0), -1).float()\n",
    "                # if use_cuda:\n",
    "                #     inputs = inputs.cuda()\n",
    "                inputs = Variable(inputs)\n",
    "                hidden = self.encode(inputs, train=False)\n",
    "                if loss_type==\"cross-entropy\":\n",
    "                    outputs = self.decode(hidden, binary=True)\n",
    "                else:\n",
    "                    outputs = self.decode(hidden)\n",
    "\n",
    "                valid_recon_loss = criterion(outputs, inputs)\n",
    "                valid_loss += valid_recon_loss.data * len(inputs)\n",
    "\n",
    "            print(\"#Epoch %3d: Reconstruct Loss: %.4f, Valid Reconstruct Loss: %.4f\" % (\n",
    "                epoch+1, train_loss / len(trainloader.dataset), valid_loss / len(validloader.dataset)))\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1YJc7tnj1VzH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def buildNetwork(layers, activation=\"relu\", dropout=0):\n",
    "    net = []\n",
    "    for i in range(1, len(layers)):\n",
    "        net.append(nn.Linear(layers[i-1], layers[i]))\n",
    "        if activation==\"relu\":\n",
    "            net.append(nn.ReLU())\n",
    "        elif activation==\"sigmoid\":\n",
    "            net.append(nn.Sigmoid())\n",
    "        if dropout > 0:\n",
    "            net.append(nn.Dropout(dropout))\n",
    "    return nn.Sequential(*net)\n",
    "\n",
    "def adjust_learning_rate(init_lr, optimizer, epoch):\n",
    "    lr = init_lr * (0.1 ** (epoch//100))\n",
    "    toprint = True\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group[\"lr\"]!=lr:\n",
    "            param_group[\"lr\"] = lr\n",
    "            if toprint:\n",
    "                print(\"Switching to learning rate %f\" % lr)\n",
    "                toprint = False\n",
    "\n",
    "class StackedDAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, z_dim=10, binary=True,\n",
    "        encodeLayer=[400], decodeLayer=[400], activation=\"relu\", \n",
    "        dropout=0, tied=False):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.layers = [input_dim] + encodeLayer + [z_dim]\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.encoder = buildNetwork([input_dim] + encodeLayer, activation=activation, dropout=dropout)\n",
    "        self.decoder = buildNetwork([z_dim] + decodeLayer, activation=activation, dropout=dropout)\n",
    "        self._enc_mu = nn.Linear(encodeLayer[-1], z_dim)\n",
    "        \n",
    "        self._dec = nn.Linear(decodeLayer[-1], input_dim)\n",
    "        self._dec_act = None\n",
    "        if binary:\n",
    "            self._dec_act = nn.Sigmoid()\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder(z)\n",
    "        x = self._dec(h)\n",
    "        if self._dec_act is not None:\n",
    "            x = self._dec_act(x)\n",
    "        return x\n",
    "\n",
    "    def loss_function(self, recon_x, x):\n",
    "        loss = -torch.mean(torch.sum(x*torch.log(torch.clamp(recon_x, min=1e-10))+\n",
    "            (1-x)*torch.log(torch.clamp(1-recon_x, min=1e-10)), 1))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z = self._enc_mu(h)\n",
    "\n",
    "        return z, self.decode(z)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict) \n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "    def pretrain(self, trainloader, validloader, lr=0.001, batch_size=128, num_epochs=10, corrupt=0.2, loss_type=\"cross-entropy\"):\n",
    "        trloader = trainloader\n",
    "        valoader = validloader\n",
    "        daeLayers = []\n",
    "        for l in range(1, len(self.layers)):\n",
    "            infeatures = self.layers[l-1]\n",
    "            outfeatures = self.layers[l]\n",
    "            if l!= len(self.layers)-1:\n",
    "                dae = DenoisingAutoencoder(infeatures, outfeatures, activation=self.activation, dropout=corrupt)\n",
    "            else:\n",
    "                dae = DenoisingAutoencoder(infeatures, outfeatures, activation=\"none\", dropout=0)\n",
    "            print(dae)\n",
    "            if l==1:\n",
    "                dae.fit(trloader, valoader, lr=lr, batch_size=batch_size, num_epochs=num_epochs, corrupt=corrupt, loss_type=loss_type)\n",
    "            else:\n",
    "                if self.activation==\"sigmoid\":\n",
    "                    dae.fit(trloader, valoader, lr=lr, batch_size=batch_size, num_epochs=num_epochs, corrupt=corrupt, loss_type=\"cross-entropy\")\n",
    "                else:\n",
    "                    dae.fit(trloader, valoader, lr=lr, batch_size=batch_size, num_epochs=num_epochs, corrupt=corrupt, loss_type=\"mse\")\n",
    "            data_x = dae.encodeBatch(trloader)\n",
    "            valid_x = dae.encodeBatch(valoader)\n",
    "            trainset = Dataset(data_x, data_x)\n",
    "            trloader = torch.utils.data.DataLoader(\n",
    "                trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "            validset = Dataset(valid_x, valid_x)\n",
    "            valoader = torch.utils.data.DataLoader(\n",
    "                validset, batch_size=1000, shuffle=False, num_workers=0)\n",
    "            daeLayers.append(dae)\n",
    "\n",
    "        self.copyParam(daeLayers)\n",
    "\n",
    "    def copyParam(self, daeLayers):\n",
    "        if self.dropout==0:\n",
    "            every = 2\n",
    "        else:\n",
    "            every = 3\n",
    "        # input layer\n",
    "        # copy encoder weight\n",
    "        self.encoder[0].weight.data.copy_(daeLayers[0].weight.data)\n",
    "        self.encoder[0].bias.data.copy_(daeLayers[0].bias.data)\n",
    "        self._dec.weight.data.copy_(daeLayers[0].deweight.data)\n",
    "        self._dec.bias.data.copy_(daeLayers[0].vbias.data)\n",
    "\n",
    "        for l in range(1, len(self.layers)-2):\n",
    "            # copy encoder weight\n",
    "            self.encoder[l*every].weight.data.copy_(daeLayers[l].weight.data)\n",
    "            self.encoder[l*every].bias.data.copy_(daeLayers[l].bias.data)\n",
    "\n",
    "            # copy decoder weight\n",
    "            self.decoder[-(l-1)*every-2].weight.data.copy_(daeLayers[l].deweight.data)\n",
    "            self.decoder[-(l-1)*every-2].bias.data.copy_(daeLayers[l].vbias.data)\n",
    "\n",
    "        # z layer\n",
    "        self._enc_mu.weight.data.copy_(daeLayers[-1].weight.data)\n",
    "        self._enc_mu.bias.data.copy_(daeLayers[-1].bias.data)\n",
    "        self.decoder[0].weight.data.copy_(daeLayers[-1].deweight.data)\n",
    "        self.decoder[0].bias.data.copy_(daeLayers[-1].vbias.data)\n",
    "\n",
    "    def fit(self, trainloader, validloader, lr=0.001, num_epochs=10, corrupt=0.3,\n",
    "        loss_type=\"mse\"):\n",
    "        \"\"\"\n",
    "        data_x: FloatTensor\n",
    "        valid_x: FloatTensor\n",
    "        \"\"\"\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        print(\"=====Stacked Denoising Autoencoding Layer=======\")\n",
    "        # optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.parameters()), lr=lr, momentum=0.9)\n",
    "        if loss_type==\"mse\":\n",
    "            criterion = MSELoss()\n",
    "        elif loss_type==\"cross-entropy\":\n",
    "            criterion = BCELoss()\n",
    "\n",
    "        # validate\n",
    "        total_loss = 0.0\n",
    "        total_num = 0\n",
    "        for batch_idx, (inputs, _) in enumerate(validloader):\n",
    "            inputs = inputs.view(inputs.size(0), -1).float()\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            inputs = Variable(inputs)\n",
    "            z, outputs = self.forward(inputs)\n",
    "\n",
    "            valid_recon_loss = criterion(outputs, inputs)\n",
    "            total_loss += valid_recon_loss.data * len(inputs)\n",
    "            total_num += inputs.size()[0]\n",
    "\n",
    "        valid_loss = total_loss / total_num\n",
    "        print(\"#Epoch 0: Valid Reconstruct Loss: %.4f\" % (valid_loss))\n",
    "        self.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            # train 1 epoch\n",
    "            adjust_learning_rate(lr, optimizer, epoch)\n",
    "            train_loss = 0.0\n",
    "            for batch_idx, (inputs, _) in enumerate(trainloader):\n",
    "                inputs = inputs.view(inputs.size(0), -1).float()\n",
    "                inputs_corr = masking_noise(inputs, corrupt)\n",
    "                if use_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                    inputs_corr = inputs_corr.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                inputs = Variable(inputs)\n",
    "                inputs_corr = Variable(inputs_corr)\n",
    "\n",
    "                z, outputs = self.forward(inputs_corr)\n",
    "                recon_loss = criterion(outputs, inputs)\n",
    "                train_loss += recon_loss.data*len(inputs)\n",
    "                recon_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # validate\n",
    "            valid_loss = 0.0\n",
    "            for batch_idx, (inputs, _) in enumerate(validloader):\n",
    "                inputs = inputs.view(inputs.size(0), -1).float()\n",
    "                if use_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                inputs = Variable(inputs)\n",
    "                z, outputs = self.forward(inputs)\n",
    "\n",
    "                valid_recon_loss = criterion(outputs, inputs)\n",
    "                valid_loss += valid_recon_loss.data * len(inputs)\n",
    "\n",
    "            print(\"#Epoch %3d: Reconstruct Loss: %.4f, Valid Reconstruct Loss: %.4f\" % (\n",
    "                epoch+1, train_loss / len(trainloader.dataset), valid_loss / len(validloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJIBPWdH3TVp",
    "outputId": "f039e2e2-5e2e-4f89-8a69-348b255da652"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch.utils.data\n",
    "import argparse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data for pre-training\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        Netflix('', train=True),\n",
    "        batch_size=256, shuffle=True, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        Netflix('', train=False),\n",
    "        batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "    sdae = StackedDAE(input_dim=17770, z_dim=20, binary=False,\n",
    "                      encodeLayer=[500, 500, 2000], decodeLayer=[2000, 500, 500], activation=\"relu\",\n",
    "                      dropout=0)\n",
    "    \n",
    "    # Print the pre-train model structure\n",
    "    print(sdae)\n",
    "    sdae.pretrain(train_loader, test_loader, lr=0.05, batch_size=256, #alpha, batch, epochs\n",
    "                  num_epochs=300, corrupt=0.2, loss_type=\"mse\")\n",
    "    \n",
    "    # Train the stacked denoising autoencoder\n",
    "    sdae.fit(train_loader, test_loader, lr=0.1, num_epochs=500, corrupt=0.2, loss_type=\"mse\")\n",
    "    \n",
    "    # Save the weights as pre-trained model for IDEC/DEC/DCC\n",
    "    sdae.save_model(r\"/Users/niharika/Documents/Niharika/Third Quarter/ECS 271 ML/Assignment2/sdae_netflix_weights1.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SDAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
